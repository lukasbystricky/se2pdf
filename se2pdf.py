import sys
import pdb
import logging
import re
import os
import urllib.request
import cssutils
import argparse
from weasyprint import HTML, CSS
from bs4 import BeautifulSoup

pdb.set_trace()

skip_toc = False          # do not include the the table of contents 
cleanup = False           # remove intermediary files generated by se2pdf
book_name = sys.argv[1]

base_directory = "https://raw.githubusercontent.com/standardebooks/" + book_name + "/master/"

logger = logging.getLogger('weasyprint')
logger.addHandler(logging.FileHandler('./' + book_name + '.log'))

def write_pdf():
    html=HTML(book_name + ".xhtml", encoding="utf8")
    html.write_pdf(book_name + ".pdf", stylesheets=[book_name + ".css"])

    if cleanup:
        os.remove(book_name + ".xhtml")
        os.remove(book_name + ".css")
        os.remove(book_name + ".log")

def generate_css():
    css = cssutils.parseFile("book.css")
    
    css_files = ["se.css", "core.css", "local.css"]

    for i in range(0, len(css_files)):
        stylesheet = urllib.request.urlopen(base_directory + "src/epub/css/" + css_files[i])
        css_tmp = stylesheet.read().decode("utf-8") 

        css_file = cssutils.parseString(css_tmp)
        
        for rule in css_file:
            if rule.type == 1: # only insert regular style rules
                css.insertRule(rule)

    css_str = css.cssText.decode("utf-8")
    css_str = re.sub("se:", "", css_str)                                    # remove se: specifier
    css_str = re.sub("z3998:", "", css_str)                                 # remove z3998: specifier
    css_str = re.sub("\[(?:\|type~=\")(.+?)\"\]", ".\\1", css_str)          # replace epub|type with class
    css_str = re.sub("(?:hanging-punctuation: first last);\n", "", css_str) # hanging-punctuation is not recognized by weasyprint

    f=open(book_name + ".css","w")
    f.write(css_str)
    f.close()

def generate_html():

    content = urllib.request.urlopen(base_directory + "src/epub/content.opf")
    soup = BeautifulSoup(content.read().decode("utf-8"), 'html.parser')
    files=soup.find_all("itemref")

    html_str = "<section class=\"fullpage\"> <img src=\"" + base_directory + "/src/epub/images/cover.svg\"></img></section>"
    
    exclude_from_toc = ["text/titlepage.xhtml", "text/imprint.xhtml", "text/colophon.xhtml", "text/uncopyright.xhtml"]
    frontmatter_sections = []
    tocAdded = False

    for x in files:
        temp=x.get("idref")
        temp1=temp.split(".")[0]
        body = get_body(base_directory + "src/epub/text/" + temp1 + ".xhtml")

        if body.section is not None:
            body.section["epub:type"] = body.section.get("epub:type", "") + " " + body.get("epub:type")

        if "frontmatter" in body.get("epub:type"):
            frontmatter_sections.append("text/" + body.section.get("id") + ".xhtml")
            if ("dedication" in body.section.get("epub:type")) \
                 or ("epigraph" in body.section.get("epub:type")):
                exclude_from_toc.append("text/" + body.section.get("id") + ".xhtml")
            
        elif not skip_toc and not tocAdded:
                html_str += create_toc(exclude_from_toc, frontmatter_sections)
                tocAdded = True

        html_str += str(re.sub("(?:, )?\\[?\\'\\\\n\\'(?:, )?\\]?", "", str(body.contents)))

    html_str = re.sub("(<a.*href=\")[\S]+#", "\\1#", html_str)
    html_str = html_str.replace(".xhtml", "")
    html_str = html_str.replace("text/", "#")
    html_str = html_str.replace("z3998:", "",)
    html_str = html_str.replace("epub:type", "class")
    html_str = html_str.replace("<article class=\"", "<article class=\"bodymatter ")
    html_str = html_str.replace("../", base_directory + "src/epub/")
    
    soup = BeautifulSoup(html_str, 'html.parser')
    for el in soup.find_all():
        if (len(x.get_text(strip=True))):
            x.decompose()

    f=open(book_name + ".xhtml","w")
    f.write(soup.prettify(formatter='html'))
    f.close()

def create_toc(exclude_ids, frontmatter_ids):
    toc = urllib.request.urlopen(base_directory + "src/epub/toc.xhtml")
    toc_str = toc.read().decode("utf-8")
    soup = BeautifulSoup(toc_str, 'html.parser')
    section = soup.nav

    for a_tag in soup.find_all('a'):
        if a_tag.get("href", "") in exclude_ids:
            a_tag.decompose()

    for a_tag in soup.find_all('a'):
        if a_tag.get("href", "") in frontmatter_ids:
            a_tag["class"] = a_tag.get("class", []) + ["frontmatter"]

    soup_str = str(section)
    soup_str = re.sub("<li>[\s]+?</li>", "", soup_str)
    soup_str = re.sub("(<a .*?>)[\s\n]*?(.*?)[\s\n]*?</a>", "\\2\\1</a>", soup_str)
    soup_str = re.sub("<li>([\s\S]+?)<a", "<li><span>\\1</span><a", soup_str)
    soup_str = re.sub("<nav epub:type=\"toc\" id=\"toc\">", "<section id=\"contents\">", soup_str)
    soup_str = re.sub("</nav>", "</section>", soup_str)
    return soup_str


def get_body(file):
    html = urllib.request.urlopen(file)
    html_str = html.read().decode("utf-8")
    soup = BeautifulSoup(html_str, 'html.parser')
    return soup.body
    
     
generate_css()
generate_html()
write_pdf()